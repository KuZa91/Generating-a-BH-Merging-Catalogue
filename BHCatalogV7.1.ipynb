{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> BBHs merging catalog generator 7.1</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we'll implement a notebook that, given a certain volume of sky, will return a catalog of possible BBHs merging events.\n",
    "The probability distribution implemented for the variables of the events, will be taken from the LIGO population papers [B. P. Abbott T1](https://arxiv.org/abs/1811.12940), [B. P. Abbott T2](https://arxiv.org/abs/2010.14533) and [R. Abbott et al. ](https://arxiv.org/abs/2111.03634).\n",
    "First of all, we need to import some modules ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "import statistics as st\n",
    "import random\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as scst\n",
    "from scipy import interpolate\n",
    "from scipy.integrate import quad, simpson\n",
    "from scipy.stats import poisson\n",
    "from scipy.interpolate import interp1d\n",
    "from multiprocessing import Pool, Manager, Value\n",
    "from functools import partial\n",
    "#from LISAhdf5 import LISAhdf5,ParsUnits\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Global Variables of the Simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global variables of the simulation will be set to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags for the execution modes, initialized to false, check the the FLAG selection section for additional informations and initializing them !\n",
    "\n",
    "mode_ex = False\n",
    "mode_fastmc = False\n",
    "mode_poisson = False\n",
    "SOBBH = False\n",
    "PBH = False\n",
    "LIGO_Rz = False\n",
    "Red_evol = False\n",
    "PBH_fRz = False\n",
    "PBH_fRt = False\n",
    "R_Spike = False\n",
    "sel_rs = False\n",
    "Check_Plot = False\n",
    "\n",
    "# Number of jobs spawned by the multiprocessing part of the program (use 9/10 * number of core to avoid problems)\n",
    "\n",
    "n_jobs = 80 #(mp.cpu_count() - 4)\n",
    "\n",
    "# To avoid to saturate the ram when copying the generated list to a dataframe, the process will be made by slicing the list in percentage\n",
    "\n",
    "cp_perc = 0.25 # If ram >> 16 GB put cp_perc = 1 to do the copy in one single step\n",
    "\n",
    "# Merger distribution parameters\n",
    "\n",
    "T_obs = 4. # Lisa estimated years of observation, given by T_mission*Efficiency\n",
    "max_tc = 1. # max years of coalescence time allowed for a BBH merging event\n",
    "frq_max = 1000 # Maximum frequency in hertz to which the LISA detector is sensitive\n",
    "\n",
    "# The total time used to generate the merging events, by multipling for the rate of merging, will be set to max_tc\n",
    "\n",
    "T_tot = max_tc\n",
    "\n",
    "#General Constants \n",
    "\n",
    "c = 299792.46 # speed of light in Km/sec\n",
    "G = 6.674*(10.**(-11.)) # Gravitational constant in m^3⋅kg^−1⋅s^−2\n",
    "sol_mass = 1.988e30 # Value of the Solar Mass in Kg\n",
    "H_0 = 67.8 # Hubble constant in Km/(s*MPc)\n",
    "year = 365.25*24*60*60 # Years in second \n",
    "    \n",
    "# Precision settings for the binned variables\n",
    "\n",
    "mass_prec = 400           # Binning density for the masses\n",
    "z_prec = 900              # Binning density for the redshift (affect distance density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> FLAG selection section </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way of simulating the events will be to generate the same predicted number of events in the given bin of variables phase space every time it was run...\n",
    "To give to the simulation a little bit of randomness, and increase the generation of merging events with low-probability set of variables, set the **exotic_mode** flags to true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode_ex = True # If true, in each volume of the phase space it will randomly add a number between [0,0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alternative, the events may be simulated using a fast Monte Carlo method, to do so set the **mode_fastmc** flag to true.\\\n",
    "**Beware, only one mode flag may be setup at a single time !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode_fastmc = True # If True, on each bin will generate a random uniform value and if the value is within the probability range a new event will be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, knowing that the number of events will naturally follow a Poissonian distribution, we can simulate the number of events in each phase space bin by sampling from a poisson distribution, to do this we need to set the **mode_poisson** flag to true :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_poisson = True # If True, the predicted number of events in each bin will be used to sample from a Poissonian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what concerns the catalogue, let's start by choosing if the catalogue will either be _SOBBH_ or \n",
    "_PBH_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOBBH = True # We assume the catalogue has events of stellar origin\n",
    "PBH = True # We assume the catalogue has events of primordial origin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose several different Mass Functions for the simulation, these can be selected by using the flag Mass_PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mass_PDF = 'LIGO_Fid' # With this flag, we are gonna use the LIGO fiducial model for the mass PDF in the simulation of the catalogue\n",
    "#Mass_PDF = 'LogNorm' # In this case, we are gonna use the Log-Normal mass function used to describe premordial black hole in the paper PhysRevD.96.023514\n",
    "Mass_PDF = 'Gaussian' # In this case, we are gonna assume a Gaussian mass PDF as in the paper PhysRevD.96.023514"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what concerns the merging rate, the LIGO merging rate can be choosen by setting to true the LIGO_Rz flag.\n",
    "As we know from the latest LIGO results that the best fit is with a merging rate that evolve with redshift, we can use the _Star Formation Rate_ presented in [Mangiagli et al.](https://arxiv.org/pdf/1907.12562.pdf) by setting to true the Red_evol flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red_evol = True # If true, the merging rate will evolve as a function of redshift, if false it will be assumed constant over the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Red_evol:\n",
    "    LIGO_Rz = True # If true, the code will use the fiducial LIGO R(z) merging rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to simulate a _Primordial Black Hole(PBH)_ population, we can set one of the two flags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBH_fRz = True # If true, the merging rate would assumed to be the simple power law evolution of a fixed k, where the value of R0 would be given as a fraction f of the SOBBH one\n",
    "PBH_fRt = True # If true, the merging rate would assumed to be a powerlaw of the Hubble time at redshift z, where the value of R0 would be given as a fraction f of the SOBBH one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for generical perturbations to the fiducial model, instead we can use a uniform spike-like merging rate by setting the following flag to true :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R_Spike = True # If true, the merging rate will just be an uniform value in a certain range of redshift, used for perturbations to the fiducial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we may choose to manually set up the random seeds for the simulation, in order to make that more reproducible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel_rs = True # Uncomment this to manually select the random seed of the simulation\n",
    "\n",
    "if(sel_rs):\n",
    "    np_seed = 0 # Change this value to the desired seed for numpy\n",
    "    rd_seed = 0 # Change this value to the desired seed for random\n",
    "    np.random.seed(np_seed)\n",
    "    random.seed(rd_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can choose to generate plots to check the correctness of the execution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check_Plot = True # If True, the code will generate several plots during the run to check the correctness of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Mass distribution functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the probability distribution in function of the masses.\n",
    "\n",
    "We have :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LIGO fiducial mass PDFs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if Mass_PDF == 'LIGO_Fid':\n",
    "    # Model B mass distribution function of the paper arxiv 1811.12940\n",
    "\n",
    "    # Mass Distribution parameters (values taken from the results of arxiv 1811.12940)\n",
    "\n",
    "    #m_min = 5. # Solar Masses\n",
    "    #m_max = 50. # Solar Masses\n",
    "    #alpha = 1.6 # +-1.6 Big Error !\n",
    "    #beta_q = 6.7 # +4.8 -5.9 Still Big Error !\n",
    "\n",
    "    # Function for estimating the Phase Space costant of the Mass distribution\n",
    "\n",
    "    #def ModBPS(ran_m1, ran_m2, m_min, m_max, alpha, beta_q):\n",
    "    #    \n",
    "    #    ris = 0.\n",
    "    #    \n",
    "    #    for i in range(len(ran_m1)- 1):\n",
    "    #        for j in range(len(ran_m2)- 1):\n",
    "    #            if(ran_m1[i] >= m_min and ran_m1[i] <= m_max and ran_m2[j] <= ran_m1[i] and ran_m2[j] >= m_min):\n",
    "    #                mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "    #                mid_m2 = 0.5*(ran_m2[j + 1] + ran_m2[j])\n",
    "    #                q = mid_m2/mid_m1 \n",
    "    #                ris +=  (ran_m1[i + 1] - ran_m1[i])*(ran_m2[j + 1] - ran_m2[j])*(np.power(mid_m1, (-alpha))*np.power(q, beta_q))\n",
    "    #   \n",
    "    #    return ris\n",
    "\n",
    "    # Function for the distribution in function of mass (as used in paper arxiv 1811.12940)\n",
    "\n",
    "    #def MassDistr(m1, m2, m_min, m_max, alpha, beta_q, C_norm):\n",
    "    #    if(m1 >= m_min and m1 <= m_max and m2 <= m1 and m2 >= m_min) :\n",
    "    #        q = m2/m1\n",
    "    #        return (C_norm*np.power(m1, (-alpha))*np.power(q, beta_q))\n",
    "    #    else :\n",
    "    #        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Mass_PDF == 'LIGO_Fid':\n",
    "    # Power law + Peak Mass Model of the paper arxiv 2010.14533\n",
    "\n",
    "    # Mass Distribution parameters (values taken from the results of arxiv 2111.03634)\n",
    "\n",
    "    m_min = 2.5 # + 0.67 - 0.44  minimum mass allowed by the popolation inference \n",
    "    m_max = 100. # Solar Masses, taken from the prior of the paper as no real higher mass cutoff was estimated !\n",
    "    alpha = 3.4 # + 0.58 - 0.49 Power law index\n",
    "    beta_q = 1.1 # + 1.8 - 1.3  index for m2 power law in q\n",
    "    delta_m = 7.8 #+ 1.9 - 4.0  used for the low mass smoothing function, generate peak at delta_m + m_min\n",
    "    lambda_peak = 0.039 # + 0.058 - 0.026 Intensity of the gaussian peak\n",
    "    mu_m = 34 # + 2.3 - 3.8 Location of the Gaussian peak in Solar Masses\n",
    "    sigma_m = 5.09 # +4.28 - 4.34 Solar Masses, taken from arxiv 2010.14533 as no additional claim was made on last paper\n",
    "\n",
    "    # Defining of the smoothing function for m close to the minimimum mass\n",
    "\n",
    "    def MassSmoothing(m, m_min, delta_m):\n",
    "        if(m < m_min):\n",
    "            return 0.\n",
    "        else:\n",
    "            if(m >= (m_min + delta_m)):\n",
    "                return 1.\n",
    "            else:\n",
    "                factor = np.exp((delta_m/(m - m_min)) + (delta_m/(m - m_min - delta_m)))\n",
    "                return 1./(factor + 1.)\n",
    "\n",
    "    # Defining a normalized power law distribution function, needed for the final distribution function        \n",
    "\n",
    "    def MassPowLaw(m, m_min, m_max, alpha, PL_norm):\n",
    "        if(m_min < m < m_max):\n",
    "            return (1./PL_norm)*(m**(-alpha))\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    # Estimating the Phase space of the Power law distribution using trapezoidal integration\n",
    "\n",
    "    def PowerLawPS(ran_m1, m_min, m_max, alpha):\n",
    "\n",
    "        ris = 0.\n",
    "\n",
    "        for i in range(len(ran_m1)- 1):\n",
    "           if(ran_m1[i] >= m_min and ran_m1[i] <= m_max):\n",
    "                    mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "                    ris +=  (ran_m1[i + 1] - ran_m1[i])*(np.power(mid_m1, (-alpha)))\n",
    "\n",
    "        return ris\n",
    "\n",
    "\n",
    "    # Defining a Gaussian distribution of the mass, needed for the final distribution function\n",
    "\n",
    "    def MassGauss(m, mu_m, sigma_m, GS_norm):\n",
    "        return ((1./(sigma_m*np.sqrt(2.*np.pi)))*np.exp(-0.5*((m-mu_m)/sigma_m)**2.))*1./GS_norm\n",
    "\n",
    "    # Estimating the Phase space of the Gaussian distribution using trapezoidal integration\n",
    "\n",
    "    def GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m):\n",
    "\n",
    "        ris = 0.\n",
    "\n",
    "        for i in range(len(ran_m1)- 1):\n",
    "           if(ran_m1[i] >= m_min and ran_m1[i] <= m_max):\n",
    "                    mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "                    ris +=  (ran_m1[i + 1] - ran_m1[i])*((1./(sigma_m*np.sqrt(2.*np.pi)))*np.exp(-0.5*((mid_m1-mu_m)/sigma_m)**2.))\n",
    "\n",
    "        return ris\n",
    "\n",
    "\n",
    "    # Defining the normalization constant for the q dependancy of the total mass distribution\n",
    "\n",
    "    def P2PS(ran_m2, beta_q, m_min, delta_m):\n",
    "\n",
    "        q_norm = np.linspace(0,1,len(ran_m2))\n",
    "\n",
    "        for i in range(len(ran_m1) - 1):\n",
    "\n",
    "            q_norm[i] = 0.\n",
    "\n",
    "            for j in range(i + 1):\n",
    "\n",
    "                q_norm[i] += ((0.5*(ran_m2[j] + ran_m2[j + 1])/(0.5*(ran_m2[i] + ran_m2[i + 1])))**(beta_q))*\\\n",
    "                (ran_m2[j + 1] - ran_m1[j])*MassSmoothing(0.5*(ran_m2[j] + ran_m2[j + 1]), m_min, delta_m)\n",
    "\n",
    "\n",
    "        q_norm[len(ran_m1) - 1] = q_norm[len(ran_m1) - 2]\n",
    "\n",
    "        return q_norm   \n",
    "\n",
    "\n",
    "    # Defining the total Mass distribution function as the sum of two components\n",
    "\n",
    "    def MassDistr(m1, m2, m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PL_norm, GS_norm, q_norm, Mass_PS):\n",
    "\n",
    "        if(m1 > m2):\n",
    "            return ((1. - lambda_peak)*MassPowLaw(m1, m_min, m_max, alpha, PL_norm) + \\\n",
    "                    lambda_peak*MassGauss(m1, mu_m, sigma_m, GS_norm))*MassSmoothing(m1, m_min, delta_m)*\\\n",
    "                    ((m2/m1)**(beta_q))*(1./q_norm)*MassSmoothing(m2, m_min, delta_m)*(1./Mass_PS)\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "\n",
    "    # Estimating the Phase space for the Model C Mass distribution function using trapezoidal integration\n",
    "\n",
    "    def ModCPS(ran_m1, ran_m2, m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PL_norm, GS_norm, q_norm, Mass_PS):\n",
    "\n",
    "        ris = 0.\n",
    "\n",
    "        for i in range(len(ran_m1)- 1):\n",
    "            for j in range(len(ran_m2)- 1):\n",
    "                 if(ran_m1[i] >= ran_m2[j]):\n",
    "                    mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "                    mid_m2 = 0.5*(ran_m2[j + 1] + ran_m2[j])\n",
    "                    q = mid_m2/mid_m1 \n",
    "                    ris +=  (ran_m1[i + 1] - ran_m1[i])*(ran_m2[j + 1] - ran_m2[j])*((1. - lambda_peak)\\\n",
    "                    *MassPowLaw(mid_m1, m_min, m_max, alpha, PL_norm) + lambda_peak*MassGauss(mid_m1, mu_m, sigma_m, GS_norm))\\\n",
    "                    *MassSmoothing(mid_m1, m_min, delta_m)*(q**(beta_q))*(1./q_norm[i])*MassSmoothing(mid_m2, m_min, delta_m)\\\n",
    "                    *(1./Mass_PS)\n",
    "\n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LogNormal Primordial Black Holes mass PDF</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log-Normal mass PDF is taken by [B. Carr et al.](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.96.023514), and read as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Mass_PDF == 'LogNorm':\n",
    "    # We use the following distribution for the mass\n",
    "    m_min = 2.5 # Solar Masses. Minimum value assumed for the PBH mass\n",
    "    m_max = 100. # Solar Masses. Maximum value assumed for the PBH mass\n",
    "    PBH_Mc = 75.00 # Solar masses, taken from the main paper by Bavera\n",
    "    PBH_sigmamf = 1.00 # Taken from the main paper by Bavera\n",
    "    \n",
    "    def MassDistr(m, PBH_Mc, PBH_sigmamf, LN_norm):\n",
    "        return (1./(np.sqrt(2*np.pi)*PBH_sigmamf*m))*np.exp(-(np.log(m/PBH_Mc)**2)/(2*PBH_sigmamf**2))*1./LN_norm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def LogNormPS(ran_m1, m_min, m_max, PBH_Mc, PBH_sigmamf):\n",
    "\n",
    "        ris = 0.\n",
    "\n",
    "        for i in range(len(ran_m1)- 1):\n",
    "            if(ran_m1[i] >= m_min and ran_m1[i] <= m_max):\n",
    "                    mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "                    ris +=  (ran_m1[i + 1] - ran_m1[i])*MassDistr(mid_m1, PBH_Mc, PBH_sigmamf, 1.)\n",
    "            \n",
    "        return ris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Gaussian Primordial Black Holes mass PDFs</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian mass PDF can be defined as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Mass_PDF == 'Gaussian':\n",
    "    m_min = 2.5 # Solar Masses, minimal mass assumed for the simulation\n",
    "    m_max = 250. # Solar Masses, maximal mass assumed for the simulation\n",
    "    mu_m = 50. # Location of the Gaussian peak in Solar Masses\n",
    "    sigma_m = 5. # Solar Masses, describe the width of the Gaussian PDF\n",
    "    \n",
    "    # We use the following distribution for the mass\n",
    "    \n",
    "    def MassDistr(m, mu_m, sigma_m, GS_norm):\n",
    "        return ((1./(sigma_m*np.sqrt(2.*np.pi)))*np.exp(-0.5*((m-mu_m)/sigma_m)**2.))*1./GS_norm\n",
    "    \n",
    "    # This function is to estimate the normalization constant\n",
    "    def GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m):\n",
    "\n",
    "        ris = 0.\n",
    "\n",
    "        for i in range(len(ran_m1)- 1):\n",
    "            if(ran_m1[i] >= m_min and ran_m1[i] <= m_max):\n",
    "                    mid_m1 = 0.5*(ran_m1[i + 1] + ran_m1[i])\n",
    "                    ris +=  (ran_m1[i + 1] - ran_m1[i])*MassDistr(mid_m1, mu_m, sigma_m, 1.)\n",
    "            \n",
    "        return ris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> ChirpMass function </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the mass PDF used, we are gonna need a function that return the Chirp Mass, given the mass of the two events involved in the binary merging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that return the Chirp Mass of a binary merging event\n",
    "\n",
    "def ChirpMass(m1,m2): \n",
    "   return ((m1*m2)**(3./5.))/((m1+m2)**(1./5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Redshift dependent statistic </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll need a function that allow us to convert from redshift to Gigaparsec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a function to convert from Z to GPC using Hubble Law, in order to obtain the comoving distance\n",
    "\n",
    "Omega_m = 0.3\n",
    "Omega_lambda = 0.7\n",
    "Omega_k = 0.\n",
    "zmin = 1.e-5 # to avoid SNR divergence due to extremely close events\n",
    "zmax_log = 0.1 # max z value generated in log scale\n",
    "zmax_lin = 10.0 # max z value generated in lin scale\n",
    "\n",
    "\n",
    "def H(z):\n",
    "    return np.sqrt((H_0**2.)*(Omega_m*((1. + z)**3.) + Omega_k*((1. + z)**2.) + Omega_lambda))\n",
    "\n",
    "def Z_to_Gpc(z):\n",
    "    \n",
    "    # Remove the commented part to use a linear approximation of the Hubble law for low z \n",
    "    \n",
    "    #if(zmax <= 0.5):\n",
    "    #    return ((z*c*(10**(-3)))/(H_0)) # only valid for z < 0.5\n",
    "    #else:\n",
    "        \n",
    "        Int_Z = 0.\n",
    "        span_z = np.linspace(0.,z,z_prec)\n",
    "        \n",
    "        # Beware, would not work well if span_z defined in logarithmic scale\n",
    "        \n",
    "        for i in range(len(span_z) -1):\n",
    "            mid_z = 0.5*(span_z[i] + span_z[i + 1])\n",
    "            Int_Z += (((span_z[i + 1] -  span_z[i])*c*(10**(-3)))/(H(mid_z)))\n",
    "    \n",
    "        return Int_Z\n",
    "    \n",
    "def Z_to_HubbleTime(z):\n",
    "    \n",
    "    Int_Z = 0.\n",
    "    z_max = 100.\n",
    "    span_z = np.logspace(np.log10(z),np.log10(z_max),z_prec)\n",
    "        \n",
    "    # Beware, would fail if the span z is created in logarithmic scale !\n",
    "        \n",
    "    for i in range(len(span_z) -1):\n",
    "        mid_z = 0.5*(span_z[i] + span_z[i + 1])\n",
    "        Int_Z += (span_z[i + 1] -  span_z[i])/(H(mid_z)*(1. + mid_z))\n",
    "    \n",
    "    return Int_Z          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to avoid calling that several times, we are gonna define an interpolation function over a fiducial range of values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOBBH:\n",
    "    ran_z = np.logspace(np.log10(zmin/10), np.log10(1.5*zmax_lin), z_prec*100) \n",
    "if PBH:\n",
    "    ran_z = np.linspace(0, zmax_lin, z_prec*100)\n",
    "ran_d = Z_to_Gpc(ran_z)\n",
    "dist_func = interp1d(ran_z, ran_d)\n",
    "ran_d = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that estimates the differential comoving volume in function of the redshift will be the differential comoving volume of the sphere :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the following function, the differential comoving volume in function of the redshift will be estimated as a spherical surface, it need to be integrated over dr to obtain the real volume \n",
    "\n",
    "def DeVC(z, Delta_z):\n",
    "    r = dist_func(z)\n",
    "    z_2 = z + 0.5*Delta_z\n",
    "    z_1 = z_2 - Delta_z\n",
    "    Delta_r = dist_func(z_2) - dist_func(z_1)\n",
    "    return ((4.*np.pi*(r**2.)*Delta_r)/Delta_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now define, the merging rate as a function of the redshift _z_ as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if LIGO_Rz:\n",
    "    # Constant merging rate throughout the volume as fitted in the paper arxiv 1811.12940\n",
    "\n",
    "    #def R(z):\n",
    "    #    return 53.2 # +58.5 - 27.0 Gpc^-3 yr-1 Merger rate density assumed constant over the comoving volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIGO_Rz:\n",
    "    # Function for the merging rate as described in the paper arxiv 2111.03634, the flag Red_evol will decide if adopting a merging rate the evolve with redshift (true) or not (false)\n",
    "\n",
    "    k = 2.7 # + 1.8 - 1.9  VALID FOR REDSHIFT EVOLVING POWER LAW + PEAK MODEL MASS DISTRIBUTION, total agreement with SFR\n",
    "    Corr_Rz = (((1. + 0.2)**k)/(1. + ((1. + 0.2)/2.9)**(k + 2.9)))**(-1) # Normalization factor estimated at z = 0.2\n",
    "\n",
    "    # Defining the value of R0, the 0 index will have the value for redshift evolution merging rate, the 1 index would have the one for constant merging rate\n",
    "\n",
    "    R_0 = {}\n",
    "    R_0[0] = 28.1 # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted at z = 0.2\n",
    "    R_0[1] = 23.9 # +14.9 - 8.6 GPc^-3 yr^-1 Middle value fitted using a Power Law + Peak mass model and a non evolving merging rate\n",
    "\n",
    "    def R(z):\n",
    "        if(Red_evol):\n",
    "            # This merging rate was interpolated by Angelo Ricciardone and Daniel Figueroa based on arxiv 2010.14533 and arxiv 1907.12562\n",
    "            return R_0[0]*Corr_Rz*((1. + z)**k)/(1. + ((1. + z)/2.9)**(k + 2.9))\n",
    "        else:\n",
    "            return R_0[1]        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the case of _PBH_, we start by defining the model presented in [V. Atal et al.](https://arxiv.org/abs/2201.12218) evolving with a simple power broken power law having $k = 1.1$ before $z_*$ and $k = 1.4$ after. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_fRz :\n",
    "    \n",
    "    zmin = 1.e-5 # to avoid SNR divergence due to extremely close events\n",
    "    zmax_log = 0.1 # max z value generated in log scale\n",
    "    zmax_lin = 10.0 # max z value generated in lin scale\n",
    "\n",
    "    # Defining the value of R0, the 0 index will have the value for redshift evolution merging rate, the 1 index would have the one for constant merging rate\n",
    "    \n",
    "    f = 0.2 # Fraction of the LIGO merging rate in the form of PBH\n",
    "    PBH_R0 = 28.1 # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted in at z = 0.2 in ligo population inference paper arxiv2111.03634\n",
    "    PBH_CorrfRz = 1./(1. + 0.2)**2.7 # normalization factor needed to express the value of the LIGO merging rate in z=0\n",
    "    \n",
    "    def R(z):\n",
    "        if(z <= 1.):\n",
    "            PBH_k = 1.1 # Value taken from arxiv 2201.12218, valid for small z !!\n",
    "            return f*PBH_R0*PBH_CorrfRz*((1. + z)**PBH_k)\n",
    "        else:\n",
    "            PBH_k = 1.4 # Value taken from arxiv 2201.12218, valid for high z !!\n",
    "            PBH_R1_corr = f*PBH_R0*PBH_CorrfRz*(((2.)**PBH_k) - ((2.)**1.1))\n",
    "            return f*PBH_R0*PBH_CorrfRz*((1. + z)**PBH_k) - PBH_R1_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can use the same model described by [S. S. Bavera et al](https://arxiv.org/pdf/2109.05836.pdf) for the redshift evolution of the merging rate. The amplitude of the perturbation can still be parametrized using the $fR$ approach as in the previous model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PBH_fRt:\n",
    "    zmin = 1.e-5 # to avoid SNR divergence due to extremely close events\n",
    "    zmax_log = 0.1 # max z value generated in log scale\n",
    "    zmax_lin = 10.0 # max z value generated in lin scale\n",
    "\n",
    "    f = 0.2 # Fraction of the LIGO merging rate in the form of PBH\n",
    "    PBH_R0 = 28.1 # +14.8 - 10.0 GPC⁻³ yr^⁻¹ Value of the merging rate fitted in at z = 0.2 in ligo population inference paper arxiv2111.03634\n",
    "    PBH_CorrfRz = 1./(1. + 0.2)**2.7 # normalization factor needed to express the value of the LIGO merging rate in z=0\n",
    "    \n",
    "    def R(z):\n",
    "        return f*PBH_R0*PBH_CorrfRz*((t_z(z)/t_0)**(-34./37.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or for a generical perturbation at a redshift bin we can use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if R_Spike:\n",
    "    Rz_min = 4.5\n",
    "    Rz_max = 5.5\n",
    "    SOBBH_SpikeAmpl = 3000.\n",
    "    \n",
    "    def R(z):\n",
    "        # Pass the amplitude in units of 1/[yr*GPc], tipically the value is between [1, 200]\n",
    "        if (z >= Rz_min and z <= Rz_max):\n",
    "            return SOBBH_SpikeAmpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better smoothen up the redshift distribution of the generated events, in the case of the LIGO fiducial merging rate, we will use the inverse uniform sampling instead of the normal uniform smoothing for the latters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not R_Spike:\n",
    "    # Generating the inverse uniform sampling function, as described by Jesus Torrado\n",
    "\n",
    "    zs = np.logspace(np.log10(zmin), np.log10(zmax_log), z_prec*2)\n",
    "    zs = np.append(zs, np.linspace(zmax_log + (zs[(z_prec*2) - 1] - zs[(z_prec*2) - 2]), zmax_lin, z_prec*100))\n",
    "    zs = np.sort(zs, kind = 'mergesort')\n",
    "    rs = np.array([dist_func(z) for z in zs])\n",
    "    vols = 4/3 * np.pi * rs**3\n",
    "    spl = interpolate.splrep(zs, vols)\n",
    "    Vz = lambda z: interpolate.splev(z, spl, der=0)\n",
    "    DerVz = lambda z: interpolate.splev(z, spl, der=1)\n",
    "    \n",
    "    if PBH_fRt:\n",
    "        t_span = Z_to_HubbleTime(zs)\n",
    "        t_0 = Z_to_HubbleTime(1.e-12) # Can't put 0 as the logarithmic scale would fail \n",
    "        t_z = interpolate.interp1d(zs, t_span)\n",
    "    \n",
    "    def pdf_z(z):\n",
    "        return R(z)*DerVz(z) * (T_tot /(1. + z))\n",
    "\n",
    "    # Estimating the cumulative distribution functions value, the integration will be done by using the quadrature method of scipy\n",
    "    \n",
    "    zm = 0.5*(zs[1::] + zs[:-1:])\n",
    "    dz = zs[1::] - zs[:-1:]\n",
    "    ZCDF_samples = zm * 0.\n",
    "    \n",
    "    for i in range(len(zm)):\n",
    "        if i == 0:\n",
    "            ZCDF_samples[i] = dz[i]*pdf_z(zm[i])\n",
    "        else:\n",
    "            ZCDF_samples[i] = ZCDF_samples[i - 1] + dz[i]*pdf_z(zm[i])\n",
    "    #ZCDF_samples = np.array([quad(pdf_z, 0., z, limit = 500, epsabs = 1e-3, epsrel =1e-3)[0] for z in zs])\n",
    "\n",
    "    # reset arrays to get back ram !\n",
    "    \n",
    "    zm = 0.\n",
    "    dz = 0.\n",
    "    rs = 0.\n",
    "    vols = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Spin distribution functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the spin amplitude distribution function as a Beta function (Default Spin model of the cited paper) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spin Amplitude Distribution parameter (values taken from the paper arxiv 2010.14533)\n",
    "\n",
    "Expected_a = 0.25 # +0.09 - 0.07\n",
    "Var_a = 0.03  # +0.02 - 0.01\n",
    "a_max = 1.\n",
    "\n",
    "\n",
    "def BetaSpinParameters(Expected_a, Var_a):\n",
    "    expec_rel = (Expected_a/(1. - Expected_a))\n",
    "    beta_a = ((expec_rel - Var_a*np.power(1. + expec_rel, 2.))/(Var_a*np.power(1. + expec_rel, 3.)))\n",
    "    alpha_a = expec_rel*beta_a\n",
    "    return alpha_a, beta_a\n",
    "\n",
    "alpha_a, beta_a = BetaSpinParameters(Expected_a, Var_a)\n",
    "\n",
    "if(alpha_a <=1 or beta_a <=1):\n",
    "    print('Error in the selection of the values for E[a] and Var[a]')\n",
    "else:\n",
    "     print(r'We obtained $alpha_a = $ ',alpha_a, r' and $beta_a = $', beta_a)\n",
    "\n",
    "# Estimating the beta function, that will be used as a normalization constant, by using the trapeze method to avoid problems in the extremes\n",
    "\n",
    "def Beta_Func(span_a, alpha_a, beta_a):\n",
    "    ris = 0.\n",
    "    for i in range(len(span_a)- 1):\n",
    "        mid_a = 0.5*(span_a[i + 1] + span_a[i])\n",
    "        ris +=  (span_a[i + 1] - span_a[i])*(np.power(mid_a,(alpha_a - 1.))*np.power((1. - mid_a),(beta_a - 1.)))\n",
    "   \n",
    "    return ris\n",
    "\n",
    "# Distribution for the spin amplitude, the beta distribution could get values bigger than 1 !\n",
    "\n",
    "def SpinModDistrib(a, alpha_a, beta_a, Beta_Val):\n",
    "     \n",
    "    return ((np.power(a, alpha_a - 1.)*np.power(1. - a, beta_a - 1.))/(Beta_Val))\n",
    "\n",
    "# Defining the inverse cumulative function for the spin amplitudes\n",
    "\n",
    "spinamp_sample = lambda N, a, b: scst.beta(a, b).rvs(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what concerns the spin tilt distribution, we will implement a mixture between a Gaussian and an isotropic function (default spin of the cited paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spin tilt Distribution parameters (values assumed considering the results of arxiv 2010.14533, see equation (6) of 1811.12940 for additional information)\n",
    "\n",
    "sigma_1 = 0.80 # + 1.08 - 0.45\n",
    "sigma_2 = 0.80 # + 1.08 - 0.45\n",
    "zeta = 0.76 # + 0.22 - 0.45\n",
    "\n",
    "# Spin orientation distribution, zeta = 1 gives a gaussian distribution centered in cos_ti = 1, zeta = 0 will return a isotropic distribution\n",
    "\n",
    "def SpinOrientDistrib(cos_t1,cos_t2, zeta, sigma_1, sigma_2):\n",
    "    prob = (1. - zeta)/(4) + ((2.*zeta)/(np.pi))*\\\n",
    "    (np.exp(-((np.power(1. - cos_t1,2.))/(2.*np.power(sigma_1,2.))))/(sigma_1*sc.erf(np.sqrt(2)/sigma_1)))\\\n",
    "    *(np.exp(-((np.power(1. - cos_t2,2.))/(2.*np.power(sigma_2,2.))))/(sigma_2*sc.erf(np.sqrt(2)/sigma_2)))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to implement the Inverse cumulative distribution function _ICDF_ even for this _PDF_ , this may be trivially done considering the symmetry of the previous _PDF_ for exchange $cos(t_1) \\rightarrow cos(t_2)$.\n",
    "Let's start by estimating the values over the phase space :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros((2000,2000))\n",
    "ran_cos_t1 = np.linspace(-1.,1.,2000)\n",
    "ran_cos_t2 = np.linspace(-1.,1.,2000)\n",
    "X, Y = np.meshgrid(ran_cos_t1, ran_cos_t2)\n",
    "\n",
    "for i in range(len(ran_cos_t1)):\n",
    "    for j in range(len(ran_cos_t2)):\n",
    "        \n",
    "        Z[j][i] = SpinOrientDistrib(X[j][i], Y[j][i], zeta, sigma_1, sigma_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now marginalize over $cos(t_2)$ to obtain $P(cos(t_1))$, due to the simmetry in the _PDF_ the latter would be equal to $P(cos(t_2))$, and hence we may estimate the common _ICDF_ on the cumulative function of the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int_Val = np.linspace(0,1,2000) # Array for the marginalization of the spin tilt distribution over cos(t2)\n",
    "Cum_Val = np.linspace(0,1,2000) # Array for the cumulative distribution of the spin tilt over cos(t_1)\n",
    "\n",
    "# For a linear scale, the Delta_cos is supposed to be constant\n",
    "\n",
    "Delta_cos = (ran_cos_t2[1] - ran_cos_t2[0])\n",
    "\n",
    "#Estimating the marginalization over cos(t2) to obtain P(cos(t1))\n",
    "\n",
    "for i in range(len(ran_cos_t1)):\n",
    "    Int_Val[i] = 0.\n",
    "    for j in range(len(ran_cos_t2)):\n",
    "        Int_Val[i] += Delta_cos*Z[j][i]\n",
    "        \n",
    "# Estimating the cumulative distribution of the resulting marginalized probability  P(cos(t1)) in function of cos(t1)\n",
    "        \n",
    "for i in range(len(ran_cos_t1)):\n",
    "    Cum_Val[i] = 0.\n",
    "    \n",
    "    if(i == 0):\n",
    "        Cum_Val[i] = Delta_cos*Int_Val[i]\n",
    "    else :\n",
    "        Cum_Val[i] = Delta_cos*Int_Val[i] + Cum_Val[i - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we renormalize the cumulative function, the sample from the _PDF_ may be easily done using the function <em>spintilt_sample</em>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To 0-1 range\n",
    "Cum_Val = (Cum_Val - min(Cum_Val)) / (max(Cum_Val) - min(Cum_Val))\n",
    "# Invert\n",
    "spl_inv_spintilt = interpolate.splrep(Cum_Val, ran_cos_t1)\n",
    "\n",
    "def spintilt_sample(N):\n",
    "    return interpolate.splev(np.random.random(N), spl_inv_spintilt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Number of events in function of the parameters </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may finally define the distribution function for the number of events :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density function for the events in function of the parameters, taken from the original paper arxiv 1811.12940 \n",
    "\n",
    "def NDistrib(z, m1, m2, Delta_z, m21_norm = []):\n",
    "    if Mass_PDF == 'LIGO_Fid':\n",
    "        n = R(z)*DeVC(z, Delta_z)*(T_tot /(1. + z)) \\\n",
    "        *MassDistr(m1, m2, m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PL_norm, GS_norm, m21_norm , Mass_PS)\n",
    "    \n",
    "    if Mass_PDF == 'LogNorm':\n",
    "        n = R(z)*DeVC(z, Delta_z)*(T_tot /(1. + z)) \\\n",
    "        *MassDistr(m1, PBH_Mc, PBH_sigmamf, LN_norm)*MassDistr(m2, PBH_Mc, PBH_sigmamf, LN_norm)\n",
    "        \n",
    "    if Mass_PDF == 'Gaussian':\n",
    "            n = R(z)*DeVC(z, Delta_z)*(T_tot /(1. + z)) \\\n",
    "            *MassDistr(m1, mu_m, sigma_m, GS_norm)*MassDistr(m2, mu_m, sigma_m, GS_norm)\n",
    "                \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Frequency of the generated events </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement a function to roughly estimate the initial orbital frequency of the event given the massess and the residual coalescence time, as taken by **equation 11 a)** of [S. Marsat and J. G. Baker](https://arxiv.org/pdf/1806.10734.pdf); from that equation we may also estimate the residual time for the events frequency to go outside of the LISA band.\n",
    "We need to remember that the coalescence time and the maximum frequency in the two following functions need to be redshifted in the source frame, we have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function given the mass of the two events and the redshifted residual time to coalescence, return the initial frequency of the event at the LISA detection time in the source frame\n",
    "def GetInitialFrequency(m1,m2,coal_T):\n",
    "    M = m1 + m2\n",
    "    ni = (m1*m2)/(M*M)\n",
    "    res = ((256.*ni)/(5.*np.power((c*(10.**3.)),5.)))*np.power((G*M*sol_mass),(5./3.))*coal_T\n",
    "    return (np.power(res,(-(3./8.)))/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function given the mass of the two events and the max frequency detectable in the source frame, return the residual time that the event will spend on the Lisa band in the source frame\n",
    "def TimeOutFrqRange(m1,m2,f_max):\n",
    "    M = m1 + m2\n",
    "    ni = (m1*m2)/(M*M)\n",
    "    res = (5.*np.power((c*(10.**3.)),5.))/(256.*ni*np.power((np.pi*f_max),(8./3.))*np.power((G*M*sol_mass),(5./3.)))\n",
    "    return res/year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Functions for the generation of the catalogue </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a function that randomly generate a name for the single events, just to make the result appear sexier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will generate a fake name for the event in the time range of the LISA mission, it will not generate events in the day 29,30,31\n",
    "\n",
    "def Gen_Event_name():\n",
    "    \n",
    "    month = random.randrange(12) + 1\n",
    "    day = random.randrange(28) + 1\n",
    "    nid = str(random.randrange(1000) + 1)\n",
    "    \n",
    "    if month < 10 :\n",
    "        month = '0'+ str(month)        \n",
    "    else:\n",
    "        month = str(month)\n",
    "        \n",
    "    if day < 10 :\n",
    "        day = '0'+ str(day)        \n",
    "    else:\n",
    "        day = str(day)    \n",
    "        \n",
    "    return \"GW\"+ str(random.randrange(34,37))+ month + day +\"NId\"+ nid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may hence define a function that will give a uniform random value in the considered phase space bin, this function will smoothen up the values of the generated catalogue in order to not have a single representant parameter for each of the considered bin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a number of events equal to the lenght of the array N_EvXBin, with random values in the phase deltavolume for each of the parameter\n",
    "\n",
    "def Gen_Events_Parameters(idx_m1, idx_m2, idx_z) :\n",
    "                                \n",
    "        ev_m1 = random.uniform(ran_m1[idx_m1],ran_m1[idx_m1 + 1])\n",
    "        ev_m2 = random.uniform(ran_m2[idx_m2],ran_m2[idx_m2 + 1])\n",
    "        \n",
    "        # By convention we impose that m1 is the biggest mass among the 2\n",
    "        \n",
    "        if(ev_m2 > ev_m1):\n",
    "            app = ev_m1\n",
    "            ev_m1 = ev_m2\n",
    "            ev_m2 = app\n",
    "        \n",
    "        \n",
    "        # Defining the redshift of the event\n",
    "        \n",
    "        if not R_Spike:\n",
    "            # We use ICDF, the next lines are to avoid overshooting of the spline function\n",
    "            ev_z = 1.5*zmax_lin\n",
    "            while(ev_z > zmax_lin):\n",
    "                ev_z = float(interpolate.splev(np.random.random(), IZCDF[idx_z]))\n",
    "        else:\n",
    "            ev_z = random.uniform(ran_z[idx_z],ran_z[idx_z + 1])\n",
    "        \n",
    "        # Now defining time to coalescence and initial frequency in the detector frame\n",
    "        \n",
    "        ev_ttilde = random.uniform(1.e-5, max_tc)/(1 + ev_z) # Coalescence time in the source frame                    \n",
    "        ev_ifrq = (1./(1. + ev_z))*GetInitialFrequency(ev_m1, ev_m2,ev_ttilde*year) # Initial frequency in the detector frame\n",
    "        \n",
    "         \n",
    "        # If the event frequency is within the LISA band the others parameters would be generated, in the other case the event is rejected \n",
    "        \n",
    "        if(ev_ifrq <= frq_max):\n",
    "            \n",
    "            BH = pd.DataFrame([[ev_z, ev_m1, ev_m2, ev_ifrq, ev_ttilde],],\\\n",
    "                            columns = ['Redshift', 'Mass1', 'Mass2', 'InitialFrequency', 'CoalTime'])\n",
    "            return BH\n",
    "        else:\n",
    "            BH = pd.DataFrame({'none' : []})\n",
    "            return BH\n",
    "            \n",
    "            \n",
    "          \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Parallelized function for the generation of the population </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, will parallelize the nested cycles by splitting them in function of the mass_1 bin, each process will then estimate the number of events for the given mass_1 bin and generate the events to append to the dataframe\n",
    "\n",
    "def init_globals(Nr,Ne,Prc):\n",
    "    global Nreal,Nev,Perc\n",
    "    Nreal = Nr\n",
    "    Nev = Ne\n",
    "    Perc = Prc\n",
    "\n",
    "def  Bin_and_Gen(d,im1) :\n",
    "    # Let's use the fact that the mass probability distribution gives 0 for m2 > m1 to only compute events for bins with m2 < m1\n",
    "    for im2 in (range(im1 + 1)):\n",
    "        \n",
    "        for iz in range(len(ran_z) - 1):\n",
    "\n",
    "            # estimating the value of NDistrib in function of the values, the value will be interpolated with the trapeze method\n",
    "            if Mass_PDF == 'LIGO_Fid':\n",
    "                \n",
    "                nstep =  NDistrib(st.mean([ran_z[iz],ran_z[iz + 1]]), st.mean([ran_m1[im1],ran_m1[im1 + 1]]),\\\n",
    "                                  st.mean([ran_m2[im2],ran_m2[im2 + 1]]), (ran_z[iz +1] - ran_z[iz]), m21_norm = q_norm[im1])\n",
    "            else:\n",
    "                \n",
    "                nstep =  2.*NDistrib(st.mean([ran_z[iz],ran_z[iz + 1]]), st.mean([ran_m1[im1],ran_m1[im1 + 1]]),\\\n",
    "                                  st.mean([ran_m2[im2],ran_m2[im2 + 1]]), (ran_z[iz +1] - ran_z[iz]))\n",
    "                                \n",
    "            # to obtain the real result of the integral, we now need to multiply by the values of the delta of all the integration variables  \n",
    "\n",
    "            nstep *= (ran_z[iz +1] - ran_z[iz])*(ran_m1[im1 + 1] - ran_m1[im1])*(ran_m2[im2 + 1] - ran_m2[im2])\n",
    "\n",
    "            # Adding the fraction of events to the cumulative sum\n",
    "\n",
    "            with Nreal.get_lock():            \n",
    "                Nreal.value += nstep \n",
    "\n",
    "            # Checking if mode_fast_mc and implementing\n",
    "\n",
    "            if(mode_fastmc):\n",
    "                if(nstep >=1.):\n",
    "                    if(nstep - round(nstep) >= 0):\n",
    "                        res = nstep - round(nstep)\n",
    "                        if(np.random.random() <= res):\n",
    "                            nstep = round(nstep) + 1.\n",
    "                    else:\n",
    "                        res = nstep + 1. - round(nstep)\n",
    "                        if(np.random.random() > res):\n",
    "                            nstep = round(nstep) - 1.\n",
    "                else:\n",
    "                    if(np.random.random() <= nstep):\n",
    "                        nstep = 1.\n",
    "\n",
    "            # Checking if mode exotic\n",
    "\n",
    "            if(mode_ex):\n",
    "                nstep += (np.random.random()*0.5)\n",
    "            \n",
    "            # Checking if mode poisson\n",
    "            \n",
    "            if(mode_poisson):\n",
    "                nstep = poisson(nstep).rvs()\n",
    "\n",
    "            # The value need to be round up to an integer\n",
    "            nstep = round(nstep)\n",
    "\n",
    "            with Nev.get_lock():\n",
    "                Nev.value += int(nstep)\n",
    "\n",
    "            # The estimated number of events will now be generated\n",
    "\n",
    "            for i in range(int(nstep)):\n",
    "                delta_BH = Gen_Events_Parameters(im1, im2, iz)\n",
    "                if(not delta_BH.empty):\n",
    "                    d.append(delta_BH)\n",
    "\n",
    "    # Increase the percentage index and print percentage\n",
    "    \n",
    "    with Perc.get_lock():\n",
    "         Perc.value += 1\n",
    "         if(Perc.value%int((len(ran_m1)-1)/10) == 0):\n",
    "            print('Percentage of completition : ',(Perc.value*100.)/(len(ran_m1)-1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Colormaps of the probability functions and check of reliability </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the colormaps of the probability functions to check how they behave, we start with the mass distribution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colormap of the mass distribution function\n",
    "\n",
    "if Check_Plot:\n",
    "    \n",
    "    if Mass_PDF == 'LIGO_Fid':\n",
    "        # Need a special mass binning to catch well the model features\n",
    "        ran_m1 = np.logspace(np.log10(m_min),np.log10(m_min + delta_m), int(mass_prec/10))\n",
    "        ran_m1 = np.append(ran_m1, np.linspace(m_min + delta_m, m_max, mass_prec))\n",
    "        ran_m1 = np.sort(ran_m1, kind = 'mergesort')\n",
    "        ran_m2 = ran_m1\n",
    "        Z = np.zeros((int(mass_prec*11/10),int(mass_prec*11/10)))\n",
    "        \n",
    "    if (Mass_PDF == 'LogNorm' or Mass_PDF == 'Gaussian'):\n",
    "        ran_m1 = np.linspace(m_min, m_max, mass_prec)\n",
    "        ran_m2 = ran_m1\n",
    "        Z = np.zeros((int(mass_prec),int(mass_prec)))\n",
    "        \n",
    "    X, Y = np.meshgrid(ran_m1, ran_m2)\n",
    "\n",
    "    # Mass distribution model B used for the simulation with the values of arxiv 1811.12940\n",
    "\n",
    "    # MassPhaseSpace = ModBPS(ran_m1, ran_m2, m_min, m_max, alpha, beta_q)\n",
    "\n",
    "    if Mass_PDF == 'LIGO_Fid':\n",
    "        # Mass distribution model C used for the simulation with the values of arxiv 2111.03634\n",
    "        PLPS = PowerLawPS(ran_m1, m_min, m_max, alpha)\n",
    "        GSPS = GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m)\n",
    "        q_norm = P2PS(ran_m2, beta_q, m_min, delta_m)\n",
    "        Mass_PS = ModCPS(ran_m1, ran_m2, m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PLPS, GSPS, q_norm, 1.)\n",
    "    \n",
    "    if Mass_PDF == 'LogNorm':\n",
    "        Mass_PS = LogNormPS(ran_m1, m_min, m_max, PBH_Mc, PBH_sigmamf)\n",
    "        \n",
    "    if Mass_PDF == 'Gaussian':\n",
    "        Mass_PS = GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m)\n",
    "                    \n",
    "    print('The integrated probability for all possible mass pairs  before normalization is : ', Mass_PS) \n",
    "\n",
    "    for i in range(len(ran_m1) -1):\n",
    "        for j in range(len(ran_m2) - 1):\n",
    "\n",
    "            # Value of the colormap for the mass distribuction used in arxiv 1811.12940, it is estimated with a trapezoid formula to avoid singularities in the extremes\n",
    "\n",
    "            # Z[j][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, (1./MassPhaseSpace))\n",
    "\n",
    "            # Value of the colormap for the mass distribuction used in arxiv 2010.14533, it is estimated with a trapezoid formula to avoid singularities in the extremes\n",
    "            \n",
    "            if Mass_PDF == 'LIGO_Fid':\n",
    "                Z[j][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, delta_m, lambda_peak,\\\n",
    "                                    mu_m, sigma_m, PLPS, GSPS, q_norm[i], Mass_PS)\n",
    "            \n",
    "            if Mass_PDF == 'LogNorm':\n",
    "                    Z[j][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]),PBH_Mc, PBH_sigmamf, Mass_PS)* MassDistr(0.5*(Y[j][i] + Y[j + 1][i]),PBH_Mc, PBH_sigmamf, Mass_PS)\n",
    "            \n",
    "            if Mass_PDF == 'Gaussian':\n",
    "                    Z[j][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), mu_m, sigma_m, Mass_PS)* MassDistr(0.5*(Y[j][i] + Y[j + 1][i]), mu_m, sigma_m, Mass_PS)\n",
    "            \n",
    "            # Fulfill the borders of the colormap by extending the nearest value\n",
    "\n",
    "            if(i == (len(ran_m1) -2)):\n",
    "                \n",
    "                #Z[j][i+1] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, (1./MassPhaseSpace))\n",
    "                \n",
    "                if Mass_PDF == 'LIGO_Fid':\n",
    "                    Z[j][i+1] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, delta_m, lambda_peak,\\\n",
    "                                          mu_m, sigma_m, PLPS, GSPS, q_norm[i], Mass_PS)\n",
    "                if Mass_PDF == 'LogNorm':\n",
    "                    Z[j][i+1] = MassDistr(0.5*(X[j][i] + X[j][i + 1]),PBH_Mc, PBH_sigmamf, Mass_PS)* MassDistr(0.5*(Y[j][i] + Y[j + 1][i]),PBH_Mc, PBH_sigmamf, Mass_PS)\n",
    "                \n",
    "                if Mass_PDF == 'Gaussian': \n",
    "                    Z[j][i+1] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), mu_m, sigma_m, Mass_PS)* MassDistr(0.5*(Y[j][i] + Y[j + 1][i]), mu_m, sigma_m, Mass_PS)                                                                                           \n",
    "            \n",
    "            if(j == (len(ran_m2) -2)):\n",
    "                \n",
    "                #Z[j+1][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, (1./MassPhaseSpace))\n",
    "                \n",
    "                if Mass_PDF == 'LIGO_Fid':                                                                                          \n",
    "                    Z[j+1][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j + 1][i]), m_min, m_max, alpha, beta_q, delta_m, lambda_peak,\\\n",
    "                                      mu_m, sigma_m, PLPS, GSPS, q_norm[i], Mass_PS)\n",
    "            \n",
    "                if Mass_PDF == 'LogNorm':\n",
    "                    Z[j+1][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]),PBH_Mc, PBH_sigmamf, Mass_PS)*MassDistr(0.5*(Y[j][i] + Y[j + 1][i]),PBH_Mc, PBH_sigmamf, Mass_PS)\n",
    "                \n",
    "                if Mass_PDF == 'Gaussian': \n",
    "                    Z[j+1][i] = MassDistr(0.5*(X[j][i] + X[j][i + 1]), mu_m, sigma_m, Mass_PS)*MassDistr(0.5*(Y[j][i] + Y[j + 1][i]), mu_m, sigma_m, Mass_PS)                                                                                           \n",
    "            \n",
    "    # Choosing values for the last cornerpoints\n",
    "                                                                                                         \n",
    "    if Mass_PDF == 'LIGO_Fid': \n",
    "        Z[len(ran_m2) - 1][len(ran_m1) -1] = MassDistr(0.5*(X[len(ran_m2) - 1][len(ran_m1) -2] + X[len(ran_m2) - 1][len(ran_m1) -1]),\\\n",
    "                                                       0.5*(Y[len(ran_m2) - 2][len(ran_m1) - 1] + Y[len(ran_m2) - 1][len(ran_m1) -1]),\\\n",
    "                                                       m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PLPS, GSPS,\\\n",
    "                                                       q_norm[len(ran_m1) -1], Mass_PS)\n",
    "    if Mass_PDF == 'LogNorm':\n",
    "        Z[len(ran_m2) - 1][len(ran_m1) -1] = MassDistr(0.5*(X[len(ran_m2) - 1][len(ran_m1) -2] + X[len(ran_m2) - 1][len(ran_m1) -1]),PBH_Mc, PBH_sigmamf, Mass_PS)*\\\n",
    "                                             MassDistr(0.5*(Y[len(ran_m2) - 2][len(ran_m1) - 1] + Y[len(ran_m2) - 1][len(ran_m1) -1]),PBH_Mc, PBH_sigmamf, Mass_PS)\n",
    "    \n",
    "    if Mass_PDF == 'Gaussian':\n",
    "        Z[len(ran_m2) - 1][len(ran_m1) -1] = MassDistr(0.5*(X[len(ran_m2) - 1][len(ran_m1) -2] + X[len(ran_m2) - 1][len(ran_m1) -1]), mu_m, sigma_m, Mass_PS)*\\\n",
    "                                             MassDistr(0.5*(Y[len(ran_m2) - 2][len(ran_m1) - 1] + Y[len(ran_m2) - 1][len(ran_m1) -1]), mu_m, sigma_m, Mass_PS)                                                                                                     \n",
    "    \n",
    "                                                                                                                                                                                                                   \n",
    "    # Plotting the countour plot\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.contourf(X, Y, Z, 250, cmap='jet')\n",
    "    plt.colorbar();\n",
    "    plt.xlabel(r'$m_1$', fontsize = 15)\n",
    "    plt.ylabel(r'$m_2$', fontsize = 15)\n",
    "    plt.title('Mass Distribution Function', fontsize = 15)\n",
    "    plt.loglog()\n",
    "    plt.savefig('MassDistrib.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we may also plot the marginalization over $m_2$ to check if we get back the $P(m_1)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "\n",
    "    Int_Val = np.linspace(0,1,len(ran_m1))\n",
    "\n",
    "    for i in range(len(ran_m1)):\n",
    "        Int_Val[i] = 0.\n",
    "        for j in range(len(ran_m2)-1):\n",
    "            Int_Val[i] += (ran_m2[j + 1] - ran_m2[j])*Z[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the integration of the mass distribution over m2 for each m1\n",
    "\n",
    "if Check_Plot:\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.plot(ran_m1, Int_Val, color = 'navy', label = 'Integrated Real Probability over m2')\n",
    "    plt.ylim(10**(-6),0.1)\n",
    "    plt.xlabel('$m1 [Solar Mass]$', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('$Int(P(m_1, m_2^*) dm_2^*) $', fontsize = 15)\n",
    "    plt.title('Integrated probability over m2')\n",
    "    plt.savefig('MarginalizedProbabilityOverM2.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what concerns the spin amplitudes, the colormap will behave as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap of the spin amplitude distribution function, it may be observed the symmetry for swaps among a_1 and a_2\n",
    "\n",
    "if Check_Plot:\n",
    "\n",
    "    Z = np.zeros((250,250))\n",
    "    ran_a1 = np.linspace(0.,a_max,250)\n",
    "    ran_a2 = np.linspace(0.,a_max,250)\n",
    "    BetaVal = Beta_Func(ran_a1, alpha_a, beta_a)\n",
    "    X, Y = np.meshgrid(ran_a1, ran_a2)\n",
    "\n",
    "    for i in range(len(ran_a1) -1):\n",
    "        for j in range(len(ran_a2) -1):\n",
    "            Z[j][i] = SpinModDistrib(0.5*(X[j][i] + X[j][i + 1]), alpha_a, beta_a, BetaVal)*SpinModDistrib(0.5*(Y[j][i] + Y[j+1][i]), alpha_a, beta_a, BetaVal)\n",
    "\n",
    "            # Fulfill the borders of the colormap by extending the nearest value\n",
    "\n",
    "            if(i == (len(ran_a1) -2)):\n",
    "                Z[j][i+1] = SpinModDistrib(0.5*(X[j][i] + X[j][i + 1]), alpha_a, beta_a, BetaVal)*SpinModDistrib(0.5*(Y[j][i] + Y[j+1][i]), alpha_a, beta_a, BetaVal)\n",
    "            if(j == (len(ran_m2) -2)):\n",
    "                Z[j+1][i] = SpinModDistrib(0.5*(X[j][i] + X[j][i + 1]), alpha_a, beta_a, BetaVal)*SpinModDistrib(0.5*(Y[j][i] + Y[j+1][i]), alpha_a, beta_a, BetaVal)\n",
    "\n",
    "    Z[len(ran_a2) - 1][len(ran_a1) -1] = SpinModDistrib(0.5*(X[len(ran_a2) - 1][len(ran_a1)-2] + X[len(ran_a2) - 1][len(ran_a1) -1]), alpha_a, beta_a, BetaVal)*SpinModDistrib(0.5*(Y[len(ran_a2) - 2][len(ran_a1) -1] + Y[len(ran_a2) - 1][len(ran_a1) -1]), alpha_a, beta_a, BetaVal)\n",
    "\n",
    "    # Plotting the countour plot\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.contourf(X, Y, Z, 250, cmap='jet')\n",
    "    plt.colorbar();\n",
    "    plt.xlabel(r'$a_1$', fontsize = 15)\n",
    "    plt.ylabel(r'$a_2$', fontsize = 15)\n",
    "    plt.title('Spin Amplitude Distribution Function', fontsize = 15)\n",
    "    plt.savefig('SpinAmpDistrib.png',dpi=500)\n",
    "\n",
    "    # Testing also the integrated total probability\n",
    "\n",
    "    totprob = 0.\n",
    "\n",
    "    for i in range(len(ran_a1)-1):\n",
    "        for j in range(len(ran_a2)-1):\n",
    "            totprob += (ran_a1[i + 1] - ran_a1[i])*(ran_a2[j + 1] - ran_a2[j])*SpinModDistrib(st.mean([ran_a1[i],ran_a1[i+1]]), alpha_a, beta_a, BetaVal)*SpinModDistrib(st.mean([ran_a2[j],ran_a2[j+1]]), alpha_a, beta_a, BetaVal)\n",
    "\n",
    "    print('The integrated probability for all possible spin amplitudes is : ', totprob)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again, we may plot the marginalization over $a_2$ to check if we get back the $P(a_1)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "\n",
    "    Int_Val = np.linspace(0,1,250)\n",
    "    for i in range(len(ran_a1)):\n",
    "        Int_Val[i] = 0.\n",
    "        for j in range(len(ran_a2)-1):\n",
    "            Int_Val[i] += (ran_a2[j + 1] - ran_a2[j])*Z[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the integration of the spin amplitude distribution over a2 for each a1\n",
    "\n",
    "if Check_Plot:\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(ran_a1, Int_Val, color = 'navy', label = 'Integrated Real Probability over a2')\n",
    "    plt.xlabel('$a1$', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('$Int(P(a_1, a_2^*) da_2^*) $', fontsize = 15)\n",
    "    plt.title('Integrated probability over a2')\n",
    "    plt.savefig('MarginalizedProbabilityOverSpinAmplitude2.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last PDF for which the colormaps has to be shown is the Spin tilt distribution, we have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap of the spin tilt distribution function, it may be observed the symmetry for swap between cos(t1) and cos(t2)\n",
    "\n",
    "if Check_Plot:\n",
    "\n",
    "    Z = np.zeros((250,250))\n",
    "    ran_cos_t1 = np.linspace(-1.,1.,250)\n",
    "    ran_cos_t2 = np.linspace(-1.,1.,250)\n",
    "    X, Y = np.meshgrid(ran_cos_t1, ran_cos_t2)\n",
    "\n",
    "    for i in range(len(ran_cos_t1) -1):\n",
    "        for j in range(len(ran_cos_t2) -1):\n",
    "\n",
    "            Z[j][i] = SpinOrientDistrib(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j+1][i]), zeta, sigma_1, sigma_2)\n",
    "\n",
    "            # Fulfill the borders of the colormap by extending the nearest value\n",
    "\n",
    "            if(i == (len(ran_cos_t1) -2)):\n",
    "                Z[j][i+1] = SpinOrientDistrib(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j+1][i]), zeta, sigma_1, sigma_2)\n",
    "            if(j == (len(ran_cos_t2) -2)):\n",
    "                Z[j+1][i] = SpinOrientDistrib(0.5*(X[j][i] + X[j][i + 1]), 0.5*(Y[j][i] + Y[j+1][i]), zeta, sigma_1, sigma_2)\n",
    "\n",
    "    Z[len(ran_cos_t2) - 1][len(ran_cos_t1) -1] = SpinOrientDistrib(0.5*(X[len(ran_cos_t2) - 1][len(ran_cos_t1) -2] + X[len(ran_cos_t2) - 1][len(ran_cos_t1) -1]), 0.5*(Y[len(ran_cos_t2) - 2][len(ran_cos_t1) -1] + Y[len(ran_cos_t2) - 1][len(ran_cos_t1) -1]), zeta, sigma_1, sigma_2)\n",
    "\n",
    "    # Plotting the countour plot\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.contourf(X, Y, Z, 250, cmap='jet')\n",
    "    plt.colorbar();\n",
    "    plt.xlabel(r'$cos(t_1)$', fontsize = 15)\n",
    "    plt.ylabel(r'$cos(t_2)$', fontsize = 15)\n",
    "    plt.title('Spin tilt angles Distribution function', fontsize = 15)\n",
    "    plt.savefig('SpinTiltDistrib.png',dpi=500)\n",
    "\n",
    "    # Testing also the integrated total probability\n",
    "\n",
    "    totprob = 0.\n",
    "\n",
    "    for i in range(len(ran_cos_t1)-1):\n",
    "        for j in range(len(ran_cos_t2)-1):\n",
    "            totprob += (ran_cos_t1[i + 1] - ran_cos_t1[i])*(ran_cos_t2[j + 1] - ran_cos_t2[j])*SpinOrientDistrib(st.mean([ran_cos_t1[i],ran_cos_t1[i + 1]]), st.mean([ran_cos_t2[j],ran_cos_t2[j + 1]]), zeta, sigma_1, sigma_2)\n",
    "\n",
    "    print('The integrated probability for all possible tilt angles is : ', totprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which once marginalized over $cos(t_2)$ gives the following $P(cos(t_1))$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "\n",
    "    Int_Val = np.linspace(0,1,250)\n",
    "    for i in range(len(ran_cos_t1)):\n",
    "        Int_Val[i] = 0.\n",
    "        for j in range(len(ran_cos_t2)-1):\n",
    "            Int_Val[i] += (ran_cos_t2[j + 1] - ran_cos_t2[j])*Z[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the integration of the spin amplitude distribution over a2 for each a1\n",
    "\n",
    "if Check_Plot:\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(ran_cos_t1, Int_Val, color = 'navy', label = 'Integrated Real Probability over $cos(t_2)$')\n",
    "    plt.xlabel('$cos(t_1)$', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('$Int(P(cos(t_1), cos(t_2)^*) d cos(t_2)^*) $', fontsize = 15)\n",
    "    plt.title('Integrated probability over $cos(t_2)$')\n",
    "    plt.savefig('MarginalizedProbabilityOverSpinTilt2.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reset the predefined array to avoid RAM consumption :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "\n",
    "    X, Y, Z = np.zeros(1),np.zeros(1),np.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Setting of the analyzed phase space </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation will be spanned over the following range of variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialization of the mass phase space\n",
    "\n",
    "if Mass_PDF == 'LIGO_Fid':\n",
    "    # Need a special mass binning to catch well the model features\n",
    "    ran_m1 = np.logspace(np.log10(m_min),np.log10(m_min + delta_m), int(mass_prec/10))\n",
    "    ran_m1 = np.append(ran_m1, np.linspace(m_min + delta_m, m_max, mass_prec))\n",
    "    ran_m1 = np.sort(ran_m1, kind = 'mergesort')\n",
    "    ran_m2 = ran_m1\n",
    "    PL_norm = PowerLawPS(ran_m1, m_min, m_max, alpha)\n",
    "    GS_norm = GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m) \n",
    "    q_norm = P2PS(ran_m2, beta_q, m_min, delta_m)\n",
    "    Mass_PS = ModCPS(ran_m1, ran_m2, m_min, m_max, alpha, beta_q, delta_m, lambda_peak, mu_m, sigma_m, PL_norm, GS_norm, q_norm, 1.)\n",
    "\n",
    "if (Mass_PDF == 'LogNorm' or Mass_PDF == 'Gaussian'):\n",
    "    ran_m1 = np.linspace(m_min, m_max, mass_prec)\n",
    "    ran_m2 = ran_m1    \n",
    "    \n",
    "if Mass_PDF == 'LogNorm':\n",
    "    LN_norm = LogNormPS(ran_m1, m_min, m_max, PBH_Mc, PBH_sigmamf)\n",
    "        \n",
    "if Mass_PDF == 'Gaussian':\n",
    "    GS_norm = GaussPS(ran_m1, m_min, m_max, mu_m, sigma_m)    \n",
    "\n",
    "# Initialization of the distance phase space !    \n",
    "    \n",
    "if not R_Spike:\n",
    "    # We will use a logarithmic scale up to z = zmax_log and a linear from zmax_log to zmax_lin to represent the fiducial LIGO model\n",
    "    ran_z = np.logspace(np.log10(zmin), np.log10(zmax_log), int(z_prec/10))\n",
    "    ran_z = np.append(ran_z, np.linspace(zmax_log + (ran_z[int(z_prec/10)-1] - ran_z[int(z_prec/10)-2]), zmax_lin, z_prec))\n",
    "    ran_z = np.sort(ran_z, kind = 'mergesort')\n",
    "    # We need to recompute the inverse cumulative distribution functions for the redshift population\n",
    "\n",
    "    IZCDF ={}\n",
    "\n",
    "    for iz in range(len(ran_z) - 1):\n",
    "            idx_zmin = (np.abs(zs - ran_z[iz])).argmin()\n",
    "            idx_zmax = (np.abs(zs - ran_z[iz + 1])).argmin()\n",
    "\n",
    "            ZCDF_slice = ZCDF_samples[idx_zmin:idx_zmax]\n",
    "\n",
    "            # To 0-1 range\n",
    "\n",
    "            ZCDF_slice = (ZCDF_slice - min(ZCDF_slice)) / (max(ZCDF_slice) - min(ZCDF_slice))\n",
    "            ZCDF_slice = np.sort(ZCDF_slice, kind = 'mergesort')\n",
    "            # Invert\n",
    "\n",
    "            IZCDF[iz] = interpolate.splrep(ZCDF_slice, zs[idx_zmin:idx_zmax]) # Values need to be sorted from smallest to biggest !!!\n",
    "\n",
    "else:\n",
    "    Rz_min = 2.5\n",
    "    Rz_max = 3.5\n",
    "    ran_z = np.linspace(Rz_min,Rz_max, int(z_prec/10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the result will be saved in the BHCat dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PBH:\n",
    "    BHCat = pd.DataFrame(columns=['Redshift', 'Mass1', 'Mass2', 'InitialFrequency', 'InBandTime', 'EclipticLongitude', 'EclipticLatitude', 'Inclination', 'Polarization', 'InitialPhase', 'CoalTime', 'Distance', 'Spin1', 'Spin2', 'AzimuthalAngleOfSpin1', 'AzimuthalAngleOfSpin2'])\n",
    "if PBH:\n",
    "    BHCat = pd.DataFrame(columns=['Redshift', 'Mass1', 'Mass2', 'InitialFrequency', 'EclipticLongitude', 'EclipticLatitude', 'Inclination', 'Polarization', 'InitialPhase', 'CoalTime', 'Distance', 'Spin1', 'Spin2', 'AzimuthalAngleOfSpin1', 'AzimuthalAngleOfSpin2'])\n",
    "# The Lisa Dataframe will be saved without name to avoid unnecessary ram consumption\n",
    "\n",
    "SOBBHsunits = {\n",
    "\n",
    "'Redshift': 'Unit',\\\n",
    "    \n",
    "'Mass1': 'SolarMass',\\\n",
    "    \n",
    "'Mass2': 'SolarMass',\\\n",
    "\n",
    "'InitialFrequency' : 'Hertz',\\\n",
    "    \n",
    "'InBandTime' : 'Years',\\\n",
    "\n",
    "'EclipticLongitude' : 'Radian',\\\n",
    "    \n",
    "'EclipticLatitude' : 'Radian',\\\n",
    "    \n",
    "'Inclination' : 'Radian',\\\n",
    "\n",
    "'Polarization' : 'Radian',\\\n",
    "    \n",
    "'InitialPhase' : 'Radian',\\\n",
    "    \n",
    "'CoalTime' : 'Years',\\\n",
    "    \n",
    "'Distance' : 'GigaParsec',\\\n",
    "\n",
    "'Spin1' : 'Unit',\\\n",
    "    \n",
    "'Spin2' : 'Unit',\\\n",
    "    \n",
    "'AzimuthalAngleOfSpin1' : 'Radian',\\\n",
    "    \n",
    "'AzimuthalAngleOfSpin2' : 'Radian'    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example of a line of the BHCat is as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the format of an added merging event\n",
    "#BH= pd.DataFrame([[0.2,2 4.6, 18.3, 0.2, 128.5, -0.9, 4.25, 2.4, 1.6, 5.2, 251., 0.8, 0.2, 0.15, -0.2,0.4],], columns=['EventName', 'Redshift', 'Mass1', 'Mass2', 'InitialFrequency', 'EclipticLongitude', 'EclipticLatitude', 'Inclination', 'Polarization', 'InitialPhase', 'CoalTime', 'Distance', 'Spin1', 'Spin2', 'AzimuthalAngleOfSpin1', 'AzimuthalAngleOfSpin2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Main body of the simulation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may finally launch the pipeline to generate the merging events in the considered volume :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if flags variable are correct\n",
    "\n",
    "if((mode_fastmc and mode_ex) or (mode_fastmc and mode_poisson) or (mode_poisson and mode_ex)):\n",
    "    print('!! WARNING !! The simulation is not able to run with both mode flags on, it will be setted by default to mode_poisson !')\n",
    "    mode_ex = False\n",
    "    mode_fastmc = False\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':                                    \n",
    "    # start the worker processes equals to n_jobs\n",
    "    print('Percentage of completition : ',0., ' %')\n",
    "    Nev = Value('i', 0)\n",
    "    Perc = Value('i', 0)\n",
    "    Nreal = Value('d', 0.)\n",
    "    with Pool(processes = n_jobs, initializer = init_globals, initargs = (Nreal,Nev, Perc)) as pool:\n",
    "        manager = Manager()\n",
    "        d = manager.list() \n",
    "        par_func = partial(Bin_and_Gen, d)\n",
    "        pool.map(par_func, range(len(ran_m1)-1))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "                                                       \n",
    "print('During the simulation, ', Nev.value, ' merging events where generated over the ',int(round(Nreal.value)), ' predicted !')\n",
    "\n",
    "N_data = len(d)\n",
    "\n",
    "print('Among the ', Nev.value, ' merging events generated ',Nev.value - len(d),' of the generated events were rejected as outisde of the Lisa band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now save the generated list to a dataframe and reset the value of said list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('The generated list will now be copied to a dataframe !')\n",
    "Perc.value = 0\n",
    "while (len(d) > 0):\n",
    "    slc_end = int((cp_perc*N_data)+1)\n",
    "    delta_BH = pd.concat(d[0:slc_end], ignore_index=True)\n",
    "    BHCat = pd.concat([BHCat,delta_BH], sort= False, ignore_index = True)\n",
    "    del d[0:slc_end]\n",
    "    Perc.value += 1\n",
    "    if((Perc.value*cp_perc*100)%10 == 0):\n",
    "        print('Percentage of copying : ',(Perc.value*cp_perc*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and add to the generated dataframe all the missing variables by using fast vectorized operations :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BHCat['Redshift'] = BHCat.Redshift.astype(np.float16)\n",
    "BHCat['Mass1'] = BHCat.Mass1.astype(np.float16)\n",
    "BHCat['Mass2'] = BHCat.Mass2.astype(np.float16)\n",
    "BHCat['InitialFrequency'] = BHCat.InitialFrequency.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BHCat['Distance'] = (1. + BHCat.Redshift)*dist_func(BHCat.Redshift)           # Estimate the distance from the redshift using Hubble Law\n",
    "BHCat['InitialPhase'] = np.random.rand(len(BHCat.Mass1))*2.*np.pi             # Random value between 0 and 2pi\n",
    "if not PBH:\n",
    "    BHCat['InBandTime'] = (BHCat['CoalTime']- TimeOutFrqRange(BHCat['Mass1'], BHCat['Mass2'], frq_max*(1. + BHCat.Redshift))*(1. + BHCat.Redshift)) # Estimating the residual time of the event in the LISA frequency band\n",
    "    BHCat['InBandTime'] = BHCat.InBandTime.astype(np.float16)\n",
    "BHCat['Spin1'] = spinamp_sample(len(BHCat.Mass1), alpha_a, beta_a)            # Drawing spin1 amplitudes from the cumulative inverse function\n",
    "BHCat['Spin2'] = spinamp_sample(len(BHCat.Mass1), alpha_a, beta_a)            # Drawing spin2 amplitudes from the cumulative inverse function\n",
    "BHCat['AzimuthalAngleOfSpin1'] = np.arccos(spintilt_sample(len(BHCat.Mass1))) # Drawing spin tilt 1 from the cumulative inverse function\n",
    "BHCat['AzimuthalAngleOfSpin2'] = np.arccos(spintilt_sample(len(BHCat.Mass1))) # Drawing spin tilt 2 from the cumulative inverse function\n",
    "BHCat['EclipticLatitude'] = np.arcsin(2 * np.random.random(len(BHCat.Mass1)) - 1)    # Random value between -pi/2 and pi/2\n",
    "BHCat['EclipticLongitude'] = np.random.rand(len(BHCat.Mass1))*2.*np.pi         # Random value between 0 and 2pi\n",
    "BHCat['Inclination'] = np.arccos(2 * np.random.random(len(BHCat.Mass1)) - 1)   # Random value between 0 and pi\n",
    "BHCat['Polarization'] = np.random.rand(len(BHCat.Mass1))*2.*np.pi              # Random value between 0 and 2pi\n",
    "BHCat['CoalTime'] *= (1. + BHCat.Redshift) # CoalTime back to detector frame\n",
    "BHCat['CoalTime'] = BHCat.CoalTime.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may finally save the complete dataframe by running :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the name of the catalogue\n",
    "\n",
    "if(mode_ex):\n",
    "    df_nm = 'CatalogueExotic.h5'        \n",
    "else:\n",
    "    if(mode_fastmc):\n",
    "        df_nm = 'CatalogueMC.h5'       \n",
    "    else:\n",
    "        df_nm = 'Catalogue.h5'\n",
    "\n",
    "if Mass_PDF == 'LIGO_Fid':\n",
    "    df_nm = 'Fiducial' + df_nm\n",
    "if Mass_PDF == 'LogNorm':\n",
    "    df_nm = 'LogNormMc' + str(PBH_Mc)+ 'SigMf'+ str(PBH_sigmamf) + df_nm\n",
    "if Mass_PDF == 'Gaussian':\n",
    "    df_nm = 'GaussianMu' + str(mu_m)+ 'SigM' + str(sigma_m)+ df_nm\n",
    "    \n",
    "if PBH_fRz:\n",
    "    df_nm = 'f' + str(int(f*100)) + 'Rz' + df_nm\n",
    "if PBH_fRt:\n",
    "    df_nm = 'f' + str(int(f*100)) + 'Rt' + df_nm    \n",
    "if SOBBH:\n",
    "    df_key = 'SOBBH'\n",
    "    \n",
    "if PBH:\n",
    "    df_key = 'PBH'\n",
    "\n",
    " \n",
    "df_nm = df_key + df_nm\n",
    "\n",
    "# Saving the Dataframe\n",
    "        \n",
    "BHCat.to_hdf(df_nm, df_key, mode='w')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analysis of the generated catalogue </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataframe and sort by frequency to see over which values the frequencies spanned :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BHCat = pd.read_hdf(df_nm, df_key)\n",
    "BHCat.sort_values(by=['InitialFrequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The redshift distribution of the generated dataframe is as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(BHCat['Redshift'], bins = int(z_prec/10), label = 'Frequency per Bin')\n",
    "    plt.xlabel('Redshift [z]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('RedshiftDistrib.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the generated dataframe is following the density distributions, let's plot a scatter plot of the masses, together with their marginalized histograms :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(BHCat['Mass1'], BHCat['Mass2'], s=2)\n",
    "    plt.xlabel('Mass 1 [Solar Mass]', fontsize = 15)\n",
    "    plt.ylabel('Mass 2 [Solar Mass]', fontsize = 15)\n",
    "    plt.savefig('MassesScatterplot.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(BHCat['Mass1'], bins = int(mass_prec/5), label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Mass 1 [Solar Masses]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Mass_1_Hist.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    log_bin = np.logspace(np.log10(m_min),np.log10(m_max),250)\n",
    "    plt.hist(BHCat['Mass1'], bins = log_bin, label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Mass 1 [Solar Masses]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Mass_1_Hist_log.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(BHCat['Mass2'], bins = int(mass_prec/5), label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Mass 2 [Solar Masses]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Mass_2_Hist.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    log_bin = np.logspace(np.log10(m_min),np.log10(m_max),250)\n",
    "    plt.hist(BHCat['Mass2'], bins = log_bin, label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Mass 2 [Solar Masses]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Mass_2_Hist_log.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the same for the spin amplitude and orientation, we start with a scatter plot for the previous:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(BHCat['Spin1'], BHCat['Spin2'], s=2)\n",
    "    plt.xlabel('Firts spin amplitude a1', fontsize = 15)\n",
    "    plt.ylabel('Second spin amplitude a2', fontsize = 15)\n",
    "    plt.savefig('FrqHist.png',dpi=500)\n",
    "    plt.savefig('SpinAmplitudesScatterplot.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(BHCat['AzimuthalAngleOfSpin1'], BHCat['AzimuthalAngleOfSpin2'], s=2)\n",
    "    plt.xlabel('Firts spin orientation angle [rad]', fontsize = 15)\n",
    "    plt.ylabel('Second spin orientation angle [rad]', fontsize = 15)\n",
    "    plt.xlim(0,np.pi)\n",
    "    plt.ylim(0,np.pi)\n",
    "    plt.savefig('SpinTiltsScatterplot.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and their corrispettive histograms, we will plot only the ones for the bigger mass BH as it would be the same for the lower mass :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(BHCat['Spin1'], bins = 250, label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Spin Amplitude 1 ', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Spin_Ampl_1_Hist.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.hist(np.cos(BHCat['AzimuthalAngleOfSpin1']), bins = 250, label = 'Frequency per Bin', density = True)\n",
    "    plt.xlabel('Spin Tilt 1 [rad]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('Spin_Tilt_1_Hist.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the properties of the generated dataframe may be observed from an histogram of the frequencies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Check_Plot:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    log_bin = np.logspace(-4,0,1000)\n",
    "    plt.hist(BHCat['InitialFrequency'], bins = log_bin, label = 'Frequency per Bin')\n",
    "    plt.xlabel('Frequency [Hz]', fontsize = 15)\n",
    "    plt.xlim(10**(-4),1)\n",
    "    plt.axvspan(frq_max-0.001, frq_max + 0.001, 0, 1, color = 'black', label = 'LISA sensitivity cutoff')\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('FrqHist.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and an hystogram showing the time of each event inside the LISA band :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Check_Plot and not PBH:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    log_bin = np.logspace(-4,np.log10(max_tc),1000)\n",
    "    plt.hist(BHCat['InBandTime'], bins = log_bin, label = 'Frequency per Bin')\n",
    "    plt.xlabel('Residual time in LISA frequency band [years]', fontsize = 15 )\n",
    "    plt.legend(loc = 1)\n",
    "    plt.ylabel('Number of occurrencies', fontsize = 15)\n",
    "    plt.savefig('TimeInFrequencyBand.png',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving the catalogue in the LISA format </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's convert the dataframe to the standard LISA type dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LH = LISAhdf5('LISA'+df_nm)\n",
    "#pr = ParsUnits()\n",
    "\n",
    "#for p in list(SOBBHsunits.keys()):\n",
    "#    pr.addPar(p,BHCat[p],SOBBHsunits[p])\n",
    "\n",
    "#pr.addPar(\"SourceType\",df_key, \"name\")    \n",
    "#LH.addSource('SOBBH',pr, overwrite=True)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
